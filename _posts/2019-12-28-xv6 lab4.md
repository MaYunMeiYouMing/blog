---
layout: post
title:  "6.828的lab4实验"
date:   2019-12-28 21:31:01 +0800
categories: OS
tag: xv6
---

* content
{:toc}

>本文为原创

## Part A：多处理器支持和多任务协作

在本实验的第一部分，你需要扩展JOS以使其能在多处理器系统上运行，然后实现一些新的JOS内核系统调用，以允许用户级环境创建其他新环境（环境类似进程）。你还需要实现协作轮转调度(cooperative round-robin scheduling)，在当前环境自愿放弃CPU（或退出）时，允许内核从一种环境切换到另一种环境。在之后的C部分，您将实现抢占式调度，该调度使内核可以在经过一定时间后从环境重新获得对CPU的控制，即使环境不合作也是如此。

### 多处理器支持

JOS支持“symmetric multiprocessing”（SMP），SMP是一种多处理器模型，其中所有CPU都具有对系统资源（例如内存和I/O总线）的同等访问权限。尽管所有CPU在SMP中在功能上都是相同的，但是在引导过程中，它们可以分为两种类型：引导处理器（BSP）负责初始化系统和引导操作系统；只有在操作系统启动并运行后，BSP才会激活应用程序处理器（AP）。BSP的处理器是由硬件和BIOS决定的。到目前为止，你现在所有的JOS代码都已在BSP上运行。

在SMP系统中，每个CPU都有一个local APIC（LAPIC）单元。LAPIC单元负责在整个系统中传递中断。LAPIC还为其连接的CPU提供唯一的标识符。在本实验中，我们利用LAPIC单元的以下基本功能（在kern/lapic.c中）：

+ 读取LAPIC标识符（APIC ID）以了解我们的代码当前在哪个CPU上运行（请参阅cpunum()）。

+ 从BSP发送启动处理器间中断（IPI）到AP以启动其他CPU（请参阅lapic_startap()）。

+ 在C部分中，我们对LAPIC的内置计时器进行编程，通过触发时钟中断来支持抢先式多任务处理（请参阅apic_init()）。

处理器使用memory-mapped I/O (MMIO)访问其LAPIC。在MMIO中，一部分物理内存会直接存入某些I/O设备的寄存器，因此通常用于访问内存的相同加载/存储指令可使用设备寄存器访问。你已经在物理地址0xA0000上看到了一个IO区域（我们使用它来写入VGA显示缓冲区），输出缓冲区是0xf00b8000。LAPIC位于一个从地址0xFE000000（比4GB少32MB）开始的区域中，因此对于我们来说，直接映射到KERNBASE的空间很大。在JOS虚拟内存中映射了一个4MB空间的`MMIOBASE`，所以我们有地方映射这样的设备。由于以后的实验会介绍更多的MMIO区域，因此您需要编写一个简单的函数来从该区域分配空间并将设备内存映射到该区域。

#### Exercise 1

在`kern/pmap.c`中实现`mmio_map_region`。要了解其用法，请查看`kern/lapic.c`中`lapic_init`的开头。在运行mmio_map_region的测试之前，你也必须做下一个练习。

mmio_map_region的代码如下：

```
	size = ROUNDUP(size, PGSIZE);
	if((base + size > MMIOLIM) || (base + size < base))
		panic("Overflow in mmio region");
	boot_map_region(kern_pgdir, base, size, pa, PTE_PCD | PTE_PWT | PTE_W);
	base += size; //base是全局变量，同时它将保存mmio中的可用空间的首地址
	return (void*)(base - size);
```

#### Application Processor Bootstrap

在启动AP之前，BSP应该首先收集有关多处理器系统的信息，例如CPU的总数，它们的APIC ID和LAPIC单元的MMIO地址。`kern/mpconfig.c`中的`mp_init()`函数通过读取驻留在BIOS内存区域中的MP配置表来检索此信息。

`boot_aps()`函数(在`kern/init.c`中)驱动AP引导进程。AP在实模式下启动，就像引导加载程序在`boot/boot.S`中启动的方式一样，因此`boot_aps()`将AP入口代码(`kern/mpentry.S`)复制到可在实模式下寻址的内存位置。与引导加载程序不同，我们可以控制AP从何处开始执行代码。我们将代码复制到`0x7000`(`MPENTRY_PADDR`)，尽管在低位640KB的内存的任何未使用的，与页面对齐的物理地址都可以使用。

之后，`boot_aps()`通过向相应AP的LAPIC单元发送`STARTUP IPI`以及一个初始`CS:IP`地址来依次激活AP，并且AP应该从`MPENTRY_PADDR`处开始运行。`kern/mpentry.S`中的入口代码与`boot/boot.S`的入口代码非常相似。进行一些简单的设置后，它将使AP进入启用分页的保护模式，然后调用C设置例程`mp_main()`(也在`kern/init.c`中)。`boot_aps()`等待AP在其`struct CpuInfo`的cpu_status字段中发`CPU_STARTED`信号，然后再继续唤醒下一个。

##### Exercise 2

阅读`kern/init.c`中的`boot_aps()`和`mp_main()`，以及`kern/mpentry.S`中的汇编代码。确保您了解AP引导程序的控制流转换过程。然后在`kern/pmap.c`中修改您对`page_init()`的实现，以避免将`MPENTRY_PADDR`所在的页面被添加到空闲列表中，以便我们可以安全地在该物理地址复制并运行AP引导程序代码。你的代码应该能够通过被更新的`check_page_free_list()`的测试（但可能无法通过`check_kern_pgdir()`的测试）。

page_init应该修改为下面：

```
void
page_init(void)
{
	size_t i;
	for (i = 0; i < npages; i++) {
		if(i == 0) {
			pages[i].pp_ref = 1;
			pages[i].pp_link = NULL;
		} else if(i * PGSIZE >= IOPHYSMEM && i * PGSIZE <= PADDR(boot_alloc(0))) {
			pages[i].pp_ref = 1;
			pages[i].pp_link = NULL;
		} else if((struct PageInfo *)(&(pages[i])) == pa2page(MPENTRY_PADDR)) {  
            //这里是标记MPENTRY_PADDR所在的物理页
			pages[i].pp_ref = 1;
			pages[i].pp_link = NULL;
		} else {
			pages[i].pp_ref = 0;
			pages[i].pp_link = page_free_list;
			page_free_list = &pages[i];
		}
	}
}
```

##### Question 1

对比`kern/mpentry.S`与`boot/boot.S`。请记住，`kern/mpentry.S`就像内核中的所有其他内容一样经过编译和链接，在`KERNBASE`之上运行，宏`MPBOOTPHYS`的作用是什么？为什么在`kern/mpentry.S`中有必要，但在`boot/boot.S`中却没有？换句话说，如果在`kern/mpentry.S`中省略了它，那会出什么问题？

提示：回忆一下我们在实验1中讨论的链接地址和加载地址之间的区别。

通过响应来自启动CPU的STARTUP IPI信号，来启动每个非启动CPU(`AP`)。 AP将在实模式下启动，并将`CS:IP`设置为`XY00:0000`，其中XY是随STARTUP发送的8位值。 因此，此代码必须从4096字节边界开始。

因为此代码将DS设置为零，所以它必须从低2 ^ 16字节物理内存中的地址开始运行。

boot_aps()(在init.c中)将此代码复制到`MPENTRY_PADDR`(满足上述限制)。 然后，对于每个AP，它将预先分配的每个内核堆栈的地址存储在`mpentry_kstack`中，发送`STARTUP IPI`，并等待此代码确认它已启动(在init.c中的`mp_main`中发生)。

此代码类似于boot/boot.S，除了
+ 不需要启用A20
+ 它使用`MPBOOTPHYS`来计算其绝对地址
+ 依赖于符号，而不是依靠链接器填充它们

AP是从实模式下开始运行，所以需要通过 MPBOOTPHYS 宏的转换虚拟地址，运行这部分代码。boot.S 中不需要这个转换是因为代码的本来就被加载在实模式可以寻址的地方。

#### Per-CPU State and Initialization

编写多处理器OS时，区分每个处理器专用的CPU状态和整个系统共享的全局状态是非常重要的。`kern/cpu.h`定义了CPU的大多数状态，包括存储每个CPU变量的`struct CpuInfo`结构。`cpunum()`始终返回调用它的CPU的ID，该ID可用作`cpus`这样的数组的索引。 另外，宏`thiscpu`是当前CPU的`struct CpuInfo`的简写。

这是你应注意的CPU状态：

+	Per-CPU kernel stack  
	由于多个CPU可以进入内核中，因此我们需要为每个处理器使用单独的内核堆栈，以防止它们干扰彼此的执行。数组`percpu_kstacks[NCPU][KSTKSIZE]`为NCPU的内核堆栈保留了空间。  
	
	在实验2中，你映射了BSP内核堆栈的物理内存`bootstack`，在`KSTACKTOP`的下方。 同样，在本实验中，您将把每个CPU的内核堆栈映射到该区域，其中保护页充当它们之间的缓冲区。 CPU 0的堆栈仍将从`KSTACKTOP`增长； CPU 1的堆栈将从CPU 0的堆栈底部开始的`KSTKGAP`字节后开始，依此类推。 `inc/memlayout.h`显示了映射布局。

+	Per-CPU TSS and TSS descriptor  
	为了指定每个CPU的内核堆栈所在的位置，还需要每个CPU的任务状态段(TSS)。 CPU i的TSS存储在`cpus[i].cpu_ts`中，并且相应的TSS描述符在GDT条目`gdt[(GD_TSS0 >> 3)+ i]`中定义。 在`kern/trap.c`中定义的全局ts变量将不再有用。

+ 	Per-CPU current environment pointer  
	由于每个CPU可以同时运行不同的用户进程，因此我们将符号`curenv`重新定义为引用`cpus[cpunum()].cpu_env`(或`thiscpu->cpu_env`)，它指向在当前CPU上执行的环境(代码正在运行)。

+ 	Per-CPU system registers  
	所有寄存器，包括系统寄存器，都是CPU专用的。 因此，初始化这些寄存器的指令，例如lcr3()，ltr()，lgdt()，lidt()等，必须在每个CPU上执行一次。 为此，定义了函数`env_init_percpu()`和`trap_init_percpu()`。

	除此之外，如果你在解决方案中添加了任何额外的CPU状态或执行了其他任何特定于CPU的初始化(例如，在CPU寄存器中设置新位)以挑战早期实验中的问题，请确保复制它们到每个CPU上！

##### Exercise 3

修改`mem_init_mp()`(在`kern/pmap.c`中)以映射从`KSTACKTOP`开始的每个CPU堆栈，如`inc/memlayout.h`中所示。 每个堆栈的大小为`KSTKSIZE`字节加上未映射的保护页的`KSTKGAP`字节。 你的代码应该能通过`check_kern_pgdir()`中的检查。

mem_init_mp代码如下：

```
static void mem_init_mp(void)
{
	for(int i = 0 ; i < NCPU ; i ++) {
		uintptr_t va = KSTACKTOP - i * (KSTKSIZE + KSTKGAP);
		boot_map_region(kern_pgdir, va - KSTKSIZE, KSTKSIZE, PADDR(percpu_kstacks[i]), PTE_W | PTE_P);
	}
}
```

如果在进行分支合并的时候，出现了一定问题，比如没有将调用mem_init_map函数的代码合并进来，可以在mem_init函数中添加。

##### Exercise 4

`trap_init_percpu()`(`kern/trap.c`)中的代码初始化BSP的TSS和TSS描述符。它能在实验3中工作，但在其他CPU上运行时不正确。更改代码，使其可以在所有CPU上使用。(注意：您的新代码不应再使用全局ts变量。)

代码如下：

```
void trap_init_percpu(void)
{
	// Setup a TSS so that we get the right stack when we trap to the kernel.
	thiscpu->cpu_ts.ts_esp0 = (uintptr_t)(percpu_kstacks[cpunum()] + KSTKSIZE);
	thiscpu->cpu_ts.ts_ss0 = GD_KD;
	thiscpu->cpu_ts.ts_iomb = sizeof(struct Taskstate);

	// Initialize the TSS slot of the gdt.
	gdt[(GD_TSS0 >> 3) + cpunum()] = SEG16(STS_T32A, (uint32_t) (&(thiscpu->cpu_ts)),
					sizeof(struct Taskstate) - 1, 0);
	gdt[(GD_TSS0 >> 3) + cpunum()].sd_s = 0;

	// 加载TSS选择器（与其他分段选择器一样，底部的三位是特殊的；我们将其保留为0）
	ltr(GD_TSS0 + (cpunum() << 3));

	// Load the IDT
	lidt(&idt_pd);
}
```

#### Locking

在`mp_main()`中初始化AP之后，我们当前的代码会自旋。在让AP进一步执行之前，我们需要首先解决多个CPU同时运行内核代码时的竞争状态。实现它的最简单方法的是使用大内核锁。大内核锁是单个的全局锁，每当环境进入内核模式时都会被持有，并在环境返回到用户模式时释放。在此模型中，用户模式下的环境可以在任何可用的CPU上同时运行，但是内核模式下只能运行一个环境。任何其他尝试进入内核模式的环境都必须等待。

`kern/spinlock.h`声明了大的内核锁，即`kernel_lock`。它还提供`lock_kernel()`和`unlock_kernel()`，这是获取和释放锁的快捷方式。你应该在四个位置应用大内核锁：
+ 在`i386_init()`中，在BSP唤醒其他CPU之前获取锁。
+ 在`mp_main()`中，在初始化AP之后获取锁，然后调用`sched_yield()`开始在此AP上运行环境。
+ 在`trap()`中，从用户模式发生异常时获取锁。要确定异常是在用户模式下还是内核模式下发生的，请检查`tf_cs`的低位。
+ 在`env_run()`中，在切换到用户模式之前立即释放锁。不要太早或太晚地这样做，否则会遇到竞争或死锁。

[锁 参考](https://hehao98.github.io/posts/2019/04/xv6-3/)

##### Exercise 5

如上所述，通过在适当的位置调用`lock_kernel()`和`unlock_kernel()`来应用大内核锁。

`kern/init.c`下的`i386_init`：

```
	// Acquire the big kernel lock before waking up APs
	// Your code here:
	lock_kernel();
```

`kern/init.c`下的`mp_init`:

```
	// Now that we have finished some basic setup, call sched_yield()
	// to start running processes on this CPU.  But make sure that
	// only one CPU can enter the scheduler at a time!
	//
	// Your code here:
	lock_kernel();
	sched_yield();
```

`kern/init.c`下的`trap`:

```
	if ((tf->tf_cs & 3) == 3) {
		// Trapped from user mode.
		// Acquire the big kernel lock before doing any
		// serious kernel work.
		// LAB 4: Your code here.
		assert(curenv);
```

`kern/env.c`下的`env_run`:

```
void env_run(struct Env *e)
{
	if (curenv != NULL && curenv->env_status == ENV_RUNNING)
        curenv->env_status = ENV_RUNNABLE;
    curenv = e;
    curenv->env_status = ENV_RUNNING;
    curenv->env_runs++;
    lcr3(PADDR(curenv->env_pgdir));
	unlock_kernel();
    env_pop_tf(&(curenv->env_tf));
}
```

##### Question 2

似乎使用大内核锁可以保证一次只有一个CPU可以运行内核代码。 为什么每个CPU仍需要单独的内核堆栈？ 描述一个即使使用大内核锁保护，使用共享内核堆栈也会出错的情况。

答: CPU0正在处理用户态的中断，内核栈中存储了栈帧，此时CPU1的用户态程序也发生中断。因为锁是在trap函数中被调用的，在发生中断时，会先压栈再调用trap函数。

### 轮循调度

本实验中的下一个任务是更改JOS内核，以便它可以“循环”方式在多个环境之间交替。 JOS中的循环调度工作方式如下：
+ `kern/sched.c`中的函数`sched_yield()`负责选择要运行的新环境。 它以循环方式依次搜索`envs []`数组，从先前运行的环境之后开始（如果没有先前运行的环境，则从数组的开头开始），选择状态为`ENV_RUNNABLE`的第一个环境（请参见 `inc/env.h`），然后调用`env_run(`)运行该环境。
+ `sched_yield()`绝对不能在两个CPU上同时运行相同的环境。它可以表明某个环境当前正在某些CPU（可能是当前CPU）上运行，因为该环境的状态为`ENV_RUNNING`。
+ 我们为你实现了一个新的系统调用`sys_yield()`，用户环境可以调用这个系统调用来调用内核的`sched_yield()`函数，从而自动将CPU让给其他的环境。

#### Exercise 6

如上所述，在`sched_yield()`中实现循环调度。不要忘记修改`syscall()`来调用`sys_yield()`。

确保在`mp_main`中调用`sched_yield()`。

修改`kern/init.c`以创建三个（或更多！）环境，这些环境都运行程序`user/yield.c`。

运行`make qemu`。在终止之前，您应该看到环境在彼此之间来回切换了五次，如下所示。

使用几个CPU进行测试：使用`qemu CPUS = 2`。

```
Hello, I am environment 00001000.
Hello, I am environment 00001001.
Hello, I am environment 00001002.
Back in environment 00001000, iteration 0.
Back in environment 00001001, iteration 0.
Back in environment 00001002, iteration 0.
Back in environment 00001000, iteration 1.
Back in environment 00001001, iteration 1.
Back in environment 00001002, iteration 1.
```

在yield程序退出之后，系统中将没有可运行的环境，调度程序应调用JOS内核监视器。如果以上任何一种都没有发生，请在继续操作之前先修改你的代码。

sched_yield的实现：

```
void sched_yield(void)
{
	struct Env *idle;

	// Implement simple round-robin scheduling.
	//
	// 在此环境最后一次运行env之后，以循环方式在'envs'中搜索ENV_RUNNABLE环境。
	// 切换到找到的第一个这样的环境。
	//
	// 如果没有可运行的环境，但是以前在此CPU上运行的环境仍然是ENV_RUNNING，则可以选择该环境。
	// 
	// 永远不要选择当前正在另一个CPU上运行的环境（env_status == ENV_RUNNING）。
	// 如果没有可运行的环境，只需跳至下面的代码即可停止CPU。
	// LAB 4: Your code here.
	
	idle = (curenv == NULL) ? envs : (curenv + 1);

	for(struct Env* e = envs; e != envs + NENV; e++)
	{
		if(e->env_status == ENV_RUNNABLE)
		{
			env_run(e);
			continue;
		}		
	}

	if(curenv != NULL && curenv->env_status == ENV_RUNNING)
		env_run(curenv);

	// sched_halt never returns
	sched_halt();
}
```

#### Question 3

在`env_run()`的实现中，你应该调用了`lcr3()`。 在调用`lcr3()`之前和之后，你的代码（至少应该）引用变量`e`（即`env_run`的参数）。 加载`％cr3`寄存器后，MMU使用的寻址上下文将立即更改。 但是虚拟地址（即`e`）相对于给定的地址上下文具有含义-地址上下文指定了虚拟地址映射到的物理地址。为什么在寻址切换之前和之后都可以取消对指针`e`的引用？

因为e指向的在envs中，而在创建环境的时候，envs会被复制到环境中。

#### Question 4

每当内核从一种环境切换到另一种环境时，都必须确保保存了旧环境的寄存器，以便以后可以正确还原它们。为什么？这在哪里发生？

在trap处理函数中，将栈帧地址赋值给env_tf域。

### System Calls for Environment Creation

尽管你的内核现在可以在多个用户级别的环境中运行和切换，但仍限于内核最初设置的运行环境。现在，你将实现必要的JOS系统调用，以允许用户环境创建和启动其他新的用户环境。

Unix提供`fork()`系统调用作为其进程创建原语。 Unix `fork()`复制调用进程（父进程）的整个地址空间，以创建一个新进程（子进程）。从用户空间可观察到的两个唯一区别是它们的进程ID和父进程ID（由`getpid`和`getppid`返回）。在父进程中，`fork()`返回子进程的进程ID，而在子进程中，`fork()`返回0。默认情况下，每个进程都获得自己的私有地址空间，并且另一个进程对内存的修改对其他进程都不可见。

你将提供一组不同的，更原始的JOS系统调用，以创建新的用户模式环境。通过这些系统调用，除了其他样式的环境创建之外，你还可以完全在用户空间中实现类似Unix的`fork()`。你将为JOS编写的新系统调用如下：

sys_exofork:  
	该系统调用创建了一个几乎空白的新环境：在其地址空间的用户部分中未映射任何内容，并且该环境不可运行。调用`sys_exofork`时，新环境将与父环境具有相同的寄存器状态。在父级中，sys_exofork将返回新创建的环境的envid_t（如果环境分配失败，则返回负数的错误代码）。但是，在子级中，它将返回0。（由于该子级开始时标记为不可运行，因此sys_exofork实际上不会返回该子级，直到父级通过使用...标记该子级可显式允许该操作为止。）